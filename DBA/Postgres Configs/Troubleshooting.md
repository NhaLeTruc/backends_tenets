# Rigorous database benchmarking

There could be at least few reasons why it’s so easy to fail trying to understand performance of a database system, and they usually have something to do with the inherent duality:

- It’s necessary to combine expertise from both the domain specic area and general analytics expertise.
- One have to take into account both known and unknown factors.
- Establishing a comprehensive mental model of a database is surprisingly hard and could be counter-intuitive at times.

## Known vs unknown

More than once a proper benchmark was failed due to an obscure eect not being taken care of. This even goes beyond obvious known/unknown classication, and we usually have
following:

- Known knowns, e.g. we know max_wal_size parameter plays a signicant role in PostgreSQL performance.
- Known unknowns, e.g. we know max_wal_size is important, but have no idea what is the optimal value.
- Unknown unknowns, e.g. we have no idea that we run the database on a buggy Linux Kernel version, where buered IO in a cgroup is slow due to wrong memory pressure calculation and constant page reclaiming.
- Intrinsic noise, e.g. we run the database on a disk with particularly volatile latencies.

## Database model

Everyone has a dierent way of thinking about databases, and what we’re going to talk about in this section is only one particular example I nd useful. The idea is to view a database as a complex system that could be described as a function on the phase space.

What are the dimensions of our database model? Well, that’s where things get a bit scary – there are a lot of dimensions, which could be roughly grouped into following categories:

- **Database parameters**: all conguration knobs obviously affect the system performance one way or another.
- **Hardware resources**: another obvious part, the database is going to perform dierently if you give it more memory or CPU cores.
- **Workload parameters**: what exactly load we apply is important as well, the results are going to be dierent if we do one transaction per second vs if we hit it with thousands of tps.
- **Performance results**: surprisingly, the output of our system is also a part of the model. Note, that in this article we’re talking about performance, but the very same approach could be made for anything else, e.g. one can build a model describing the database availability to verify HA properties.

Besides the fact that we got a lot of dimensions here, **some of them are not even deterministic** (mostly out of the latest category) and could be defined only as random variables with certain probability distribution, which would lead us later on the dark path of statistics.

At the end we’re of course trying to simplify things and usually, at the risk of loosing high level
interaction between parameters, **we work with much smaller models**.

## Misconfiguration

Despite millions of various subtleties there is only a handful of parameters one have to know to be on the safe side most of the time. Usually the list looks something like this:

- shared_buffers
- max_wal_size
- work_mem
- checkpoint_timeout
- checkpoint_completion_target
- wal_writer_flush_after
- checkpoint_flush_after

Of course, depending on the situation the importance of parameters could be dierent, but nevertheless there is a subset of all the options that can give you some condence. How large is this subset? One interesting white paper [2], about using ML for database tuning, has this to say about the topic:

“[…] one can achieve a substantial portion of the performance gain from congurations generated by ML-based tuning algorithms by setting two knobs according to the DBMS’s
documentation. These two knobs control the amount of RAM for the buer pool cache and the size of the redo log le on disk.”

We can’t answer the question “how large is the subset of most important performance conguration options” generally, but at least in the context of automatically tuned database it’s quite modest and consist of shared_buffers and max_wal_size . Coincidently, if we take a look at how the Postgres95 1.01 distribution looked like (it’s still in the PostgreSQL git repository), one of the few performance relevant options that were available back then was the size of buer pool (there was no WAL logging yet):

Identifying relevant options is good, but guring out what values should we use is still an open question. In fact, it’s your responsibility as a person conducting the benchmark to adjust values based on the result, sort of feedback control loop. The usual rule of thumb is that, not taking into account higher level interactions, too small or too large value are performing worse than the sweet spot in the middle (but it doesn’t help much, as we still have to identify what is the range of too small and too large).

As a side note, I’ve heard many times when people were resisting to do this type of feedback tuning, claiming they want to test the system under the real conditions. It’s a reasonable request, but clear performance results not obscured by the noise are not the only goal – often it’s hard to imagine happy users, when the database performance is frustratingly variable. Which means the optimal approach might as well include the feedback tuning.

## Load Testing

The idea is simple to explain, but hard to implement – we would like to congure the database in such a way, that the eect interesting to us is going to be amplied, while everything else will be downplayed. To achieve that we need visibility into what’s going on: collect all available statistics, enable all the necessary logging, probably also use some static and dynamic instrumentation with whatever tool you prefer to use ( perf , bcc , bpftrace , etc). The plan is usually to prepare an initial conguration, run the test, and if the results dier from expectations or the variance is higher that the eect we would like to observe – then search through the metrics, trying to understand what the database is doing above of what you’ve asked it to do. It could be as easy as looking at the logs to see that the database is choking on vacuum because it’s miscongured, or seeing too many WALSync wait events because NVMe device trimming kicked in.

It may come as a surprise, but the load generator is as important as the tested subject itself. If you got a miscongured generator, you will probably get misleading results as well.

Fortunately we’ve got many options to chose from:

- Benchbase (former OLTPBench)
- Sysbench (more general benchmarking tool)
- YCSB (for “key-value” loads) HammerDB (cross-database tool from TPC)
- pgbench (out of the box benchmarking tool for PostgreSQL)
- Replicated live workload (the most cumbersome, but rewarding option)

Those tools are great, but they still leave room for mistakes. For example if miscongured, the benchmarking tool could be a bottleneck on its own, leading to misleading results. Even how much work the benchmarking tool is doing could aect the outcome – remember our motivational example at the beginning of the article? The mystery is simple, we ran pgbench two times, one only with aggregate statistics reporting, another with individual latency logging (and hence much more things for it to do).

## Reference

1. [statistics-and-benchmarking](https://erthalion.info/2023/12/29/statistics-and-benchmarking/)
