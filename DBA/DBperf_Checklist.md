# Database performance at scale

## Chapter 1: Introduction

This chapter hindlight the importance and complexities of database performance at scale.

## Chapter 2: Expectations

This chapter discuss how different organizations and tasks require different definitions of performance. It introduces keywords for consideration while defining your own performance definiton:

1. Workload Mix. Read/Write or CRUD Ratio.
2. Records size and type.
3. Dataset size.
4. Throughput expectations.
5. Latency expectations.
6. Concurency.
7. Demand Fluctuations (Spikes).
8. Consistency expectations.
9. ACID.
10. High Availability expectations.
11. Geographic Distribution.

## Chapter 3: Hardware and Operating System Interactions

A database’s internal architecture makes a tremendous impact on the latency it can
achieve and the throughput it can handle. Being an extremely complex piece of software,
a database doesn’t exist in a vacuum, but rather interacts with the environment, which
includes the operating system and the hardware.

This chapter discusses considerations of CPU; Memory; I/O; and Networking for databases hardware.

## Chapter 4: Algorithmic Optimizations

This chapter shares one detailed example of algorithmic optimization—from the
perspective of the engineer who led this optimization. Specifically, this chapter looks
at how the B-trees family can be used to store data in cache implementations and
other accessory and in-memory structures. This look into a representative engineering
challenge should help you better understand what tradeoffs or optimizations various
databases might be making under the hood—ideally, so you can take better advantage of
its very deliberate design decisions.

**IMPORTANT CHAPTER:** this one deals with the underlying algorithm that makes databases work.

## Chapter 5: Database Drivers

This chapter takes a look at how drivers impact performance—through the eyes of
someone who has engineered drivers for performance. It provides insight into various
ways that drivers can support efficient client-server interactions and shares tips for
getting the most out of a driver, particularly from the performance perspective. Finally,
the chapter wraps up with several considerations to keep in mind as you’re selecting
a driver.

Scalability is a measure of how well your system reacts to increased load. This load is
usually generated by clients using their drivers, so keeping the relationship between
your clients and servers sound is an important matter. The more you know about your
workloads, your clients’ behavior, and their usage patterns, the better you’re prepared to
handle both sudden spikes in traffic and sustained, long-term growth in usage.

## Chapter 6: Getting Data Closer

Location, location, location. Sometimes it’s just as important to database performance
as it is to real estate. Just as the location of a home influences how quickly it sells, the
location of where data “lives” and is processed also matters for response times and
latencies.

This chapter explores the opportunities in both of these shifts. First, it looks at
databases as compute engines with a focus on user-defined functions and user-defined
aggregates. It then goes deeper into WebAssembly, which is now increasingly being
used to implement user-defined functions and aggregates (among many other things).
Finally, the chapter ventures to the edge—exploring what you stand to gain by moving
your database servers quite close to your users, as well as what potential pitfalls you
need to negotiate in this scenario.

## Chapter 7: Infrastructure and Deployment Models

Regardless of your database selection, you may eventually hit a wall that no
engineering effort can break through: the database’s physical hardware. It makes very
little sense to have a solution engineered for performance when the hardware you throw
at it may be suboptimal. Similarly, a less performant database will likely be unable to
make efficient use of an abundance of available physical resources.

This chapter looks at critical considerations and tradeoffs when selecting CPUs,
memory, storage, and networking for your distributed database infrastructure. It
describes how different resources cooperate and how to configure the database to
deliver the best performance. Special attention is drawn to storage I/O as the most
difficult component to deal with. There’s also a close look at optimal cloud-based
deployments suitable for highly-performant distributed databases.

## Chapter 8: Topology Considerations

This chapter focuses on the topology in and of itself. How is data replicated across
geographies and datacenters? What are the risks and alternatives to taking the common
NoSQL practice of scaling out to the extreme? And what about intermediaries to your
database servers—for example, external caches, load balancers, and abstraction layers?
Performance implications of all this and more are all covered here.

Having more replicas will slow your writes (since every write must be duplicated
to replicas), but it could accelerate your reads (since more replicas will be available for
serving the same dataset). It will also allow you to maintain operations and avoid data loss in the event of node failures. Additionally, replicating data to get closer to your application and closer to your users will reduce latency, especially if your application has a highly geographically-distributed user base.

## Chapter 9: Benchmarking

The goal of this chapter is to share strategies that ease the pain slightly and, more
importantly, increase the chances that the pain pays off by helping you select options
that meet your performance needs. The chapter begins by looking at the two key types of
benchmarks and highlighting critical considerations for each objective. Then, it presents
a phased approach that should help you expose problems faster and with lower costs.
Next, it dives into the do’s and don’ts of benchmark planning, execution, and reporting, with a focus on lessons learned from the best and worst benchmarks we’ve witnessed
over the past several years. Finally, the chapter closes with a look at some less common
benchmarking approaches you might want to consider for specialized needs.

## Chapter 10: Monitoring

Databases require ongoing care and attention, especially when performance is a priority
and the data being stored is growing rapidly and/or changing frequently. Adverse events
that could place the business at risk—for example, node failures or a misbehaving
client—will inevitably occur. Given the complexity of both databases and data-intensive
applications, it’s not a matter of if some combination of factors ends up degrading
performance, but when.

## Chapter 11: Administration

A database’s automated admin operations work to keep things tight and tidy behind
the scenes, but a level of supervision is required. Databases don’t know your business
and could very naively decide to execute resource-intensive admin operations at
what’s actually a performance-critical time. This final chapter details how common
admin operations tend to impact performance. It covers the nature and severity of
representative impacts and offers some tips on how you might mitigate them.

## Appendix: A Brief Look at Fundamental Database Design Decisions

This appendix briefly touches on a number of fundamental database design decisions
that impact database performance. Why “briefly?” First, because we suspect that
many readers are already familiar with them. But, more importantly, because other
resources have covered them quite extensively, and extremely well. Honestly, there’s
not much to add. So, we’ll offer a short take on some of the most pressing decisions
that any distributed database must make, then share our top picks for learning more on
each topic.
